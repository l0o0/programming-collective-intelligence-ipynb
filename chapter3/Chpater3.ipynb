{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 发现群组\n",
    "物以类聚，人以群分，这里我们想学习如何利用事物间的联系而将它们进行分类。**数据聚类（data clustering）**是一种用以寻找紧密相关的事、人或观点，并将其可视化的方法。   \n",
    "在我们生物领域，经常把基因根据表达量信息来进行聚类，聚类得到的基因往往在生物学上有非常重要的意义。书中第一个例子是对博客用户所讨论的话题，以及他们使用的特殊词汇进行考查，根据文字的内容及用法分别进行分级；第二个例子是根据人们拥有和希望拥有的物品信息来对人群进行分类。\n",
    "开始前，介绍了**监督学习**和**无监督学习**的概念。**监督学习法（supervised learning methods）**是利用样本输入和期望输入来学习如何预测的方法，代表的有培养出网络、决策树、向量支持机、贝叶斯过滤等。**无监督学习（unsupervised learning）**也需要样品输入来进行预测，不过这里我们没有期望输出结果是怎样的，相反我们需要在一组数据中找出某种结构，适合进行探索性发现。有无监督这里可以简单看成对结果是否有相应的期望结果。无监督学习的包括聚类，负矩阵因式分解和自组织映射。\n",
    "## 3.1 博客用户进行分类\n",
    "在对博客用户进行分类时，我们分析这些博客用户的标题内容，根据内容的相似性进行分析。分析前，首先需要把这些文本信息转换成对应的数值型数据，这里书里采用的是分别统计单词在各个博客内容中出现的频数。目录中已经有文件`blogdata.txt`存储了词频信息，可以直接读取。详细获取过程可以参考书上内容和`generatefeedvector.py`。  \n",
    "聚类时先从单一元素开始（这里的元素可以看成初始群组），分别计算元素之间的联系，再把距离最近的两个元素当成一个群组。第二次计算前面得到的群组之间的联系（新组的值为旧组中值的平均值），再把距离相近的群组进行合并。一直循环，直到最后得到一个群组为止。这个过程可以利用**树状图**来进行展示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    with open(filename) as handle:\n",
    "        lines = handle.readlines()\n",
    "    # 列名\n",
    "    colnames = lines[0].strip().split('\\t')[1:]\n",
    "    rownames = []\n",
    "    data = []\n",
    "    for line in lines[1:]:\n",
    "        p = line.strip().split('\\t')\n",
    "        rownames.append(p[0])\n",
    "        data.append([float(x) for x in p[1:]])\n",
    "    return rownames, colnames, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算Pearson相关性系数\n",
    "from math import sqrt\n",
    "def pearson(v1, v2):\n",
    "    sum1 = sum(v1)\n",
    "    sum2 = sum(v2)\n",
    "    \n",
    "    sum1Sq = sum([pow(v,2) for v in v1])\n",
    "    sum2Sq = sum([pow(v,2) for v in v2])\n",
    "    \n",
    "    pSum = sum([v1[i]*v2[i] for i in range(len(v1))])\n",
    "    \n",
    "    num = pSum - (sum1*sum2/len(v1))\n",
    "    den = sqrt((sum1Sq-pow(sum1,2)/len(v1))*(sum2Sq-pow(sum2,2)/len(v1)))\n",
    "    if den == 0:\n",
    "        return 0\n",
    "    return 1.0 - num/den\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 描述聚类结构的类\n",
    "class bicluster:\n",
    "    def __init__(self, vec, left=None, right=None, distance=0.0, id=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.vec = vec\n",
    "        self.id = id\n",
    "        self.distance = distance\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hcluster(rows, distance=pearson):\n",
    "    distances = {}\n",
    "    currentclustid = -1\n",
    "    \n",
    "    clust = [bicluster(j, id=i) for i,j in enumerate(rows)]\n",
    "    \n",
    "    while len(clust) > 1:\n",
    "        lowestpair = (0,1)\n",
    "        closest = pearson(clust[0].vec, clust[1].vec)\n",
    "        \n",
    "        for i in range(len(clust)):\n",
    "            for j in range(i+1, len(clust)):\n",
    "                # 书中这里pearson写成distance，需要注意一下\n",
    "                if (clust[i].id, clust[j].id) not in distances:\n",
    "                    d = pearson(clust[i].vec, clust[j].vec)\n",
    "                    distances[(clust[i].id, clust[j].id)] = d\n",
    "                d = distances[(clust[i].id, clust[j].id)]\n",
    "                \n",
    "                if d < closest:\n",
    "                    closest = d\n",
    "                    lowestpair = (i,j)\n",
    "        \n",
    "        mergevec = [\n",
    "            (clust[lowestpair[0]].vec[i]+ clust[lowestpair[1]].vec[i])/2.0  \n",
    "            for i in range(len(clust[0].vec))\n",
    "        ]\n",
    "        \n",
    "        newcluster = bicluster(mergevec, left=clust[lowestpair[0]],\n",
    "                               right = clust[lowestpair[1]],\n",
    "                               distance=closest, \n",
    "                               id = currentclustid\n",
    "                               )\n",
    "        \n",
    "        currentclustid -= 1\n",
    "        del clust[lowestpair[1]]\n",
    "        del clust[lowestpair[0]]\n",
    "        clust.append(newcluster)\n",
    "    \n",
    "    return clust[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blognames, words, data = readfile('blogdata.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clust = hcluster(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printclust(clust, labels=None, n=0):\n",
    "    for i in range(n):\n",
    "        print \" \",\n",
    "    if clust.id < 0:\n",
    "        print '-'\n",
    "    else:\n",
    "        if labels==None:\n",
    "            print clust.id\n",
    "        else:\n",
    "            print labels[clust.id]\n",
    "    \n",
    "    # 在函数内部调用自身，形成递归。clust的结构也是一种递归的结果\n",
    "    if clust.left !=None:\n",
    "        printclust(clust.left, labels=labels, n=n+1)\n",
    "    if clust.right != None:\n",
    "        printclust(clust.right, labels=labels, n=n+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "  gapingvoid: \"cartoons drawn on the back of business cards\"\n",
      "  -\n",
      "    -\n",
      "      Schneier on Security\n",
      "      Instapundit.com\n",
      "    -\n",
      "      The Blotter\n",
      "      -\n",
      "        -\n",
      "          MetaFilter\n",
      "          -\n",
      "            SpikedHumor\n",
      "            -\n",
      "              Captain's Quarters\n",
      "              -\n",
      "                Michelle Malkin\n",
      "                -\n",
      "                  -\n",
      "                    NewsBusters.org - Exposing Liberal Media Bias\n",
      "                    -\n",
      "                      -\n",
      "                        Hot Air\n",
      "                        Crooks and Liars\n",
      "                      -\n",
      "                        Power Line\n",
      "                        Think Progress\n",
      "                  -\n",
      "                    Andrew Sullivan | The Daily Dish\n",
      "                    -\n",
      "                      Little Green Footballs\n",
      "                      -\n",
      "                        Eschaton\n",
      "                        -\n",
      "                          Talking Points Memo: by Joshua Micah Marshall\n",
      "                          Daily Kos\n",
      "        -\n",
      "          43 Folders\n",
      "          -\n",
      "            TechEBlog\n",
      "            -\n",
      "              -\n",
      "                Mashable!\n",
      "                Signum sine tinnitu--by Guy Kawasaki\n",
      "              -\n",
      "                -\n",
      "                  -\n",
      "                    Slashdot\n",
      "                    -\n",
      "                      MAKE Magazine\n",
      "                      Boing Boing\n",
      "                  -\n",
      "                    -\n",
      "                      Oilman\n",
      "                      -\n",
      "                        Online Marketing Report\n",
      "                        -\n",
      "                          Treehugger\n",
      "                          -\n",
      "                            SimpleBits\n",
      "                            -\n",
      "                              Cool Hunting\n",
      "                              -\n",
      "                                Steve Pavlina's Personal Development Blog\n",
      "                                -\n",
      "                                  -\n",
      "                                    ScienceBlogs : Combined Feed\n",
      "                                    Pharyngula\n",
      "                                  -\n",
      "                                    BuzzMachine\n",
      "                                    -\n",
      "                                      Copyblogger\n",
      "                                      -\n",
      "                                        -\n",
      "                                          The Viral Garden\n",
      "                                          Seth's Blog\n",
      "                                        -\n",
      "                                          -\n",
      "                                            Bloggers Blog: Blogging the Blogsphere\n",
      "                                            -\n",
      "                                              Sifry's Alerts\n",
      "                                              ProBlogger Blog Tips\n",
      "                                          -\n",
      "                                            -\n",
      "                                              Valleywag\n",
      "                                              Scobleizer - Tech Geek Blogger\n",
      "                                            -\n",
      "                                              -\n",
      "                                                O'Reilly Radar\n",
      "                                                456 Berea Street\n",
      "                                              -\n",
      "                                                Lifehacker\n",
      "                                                -\n",
      "                                                  Quick Online Tips\n",
      "                                                  -\n",
      "                                                    Publishing 2.0\n",
      "                                                    -\n",
      "                                                      Micro Persuasion\n",
      "                                                      -\n",
      "                                                        A Consuming Experience (full feed)\n",
      "                                                        -\n",
      "                                                          John Battelle's Searchblog\n",
      "                                                          -\n",
      "                                                            Search Engine Watch Blog\n",
      "                                                            -\n",
      "                                                              Read/WriteWeb\n",
      "                                                              -\n",
      "                                                                Official Google Blog\n",
      "                                                                -\n",
      "                                                                  Search Engine Roundtable\n",
      "                                                                  -\n",
      "                                                                    Google Operating System\n",
      "                                                                    Google Blogoscoped\n",
      "                    -\n",
      "                      -\n",
      "                        -\n",
      "                          -\n",
      "                            Blog Maverick\n",
      "                            -\n",
      "                              Download Squad\n",
      "                              -\n",
      "                                CoolerHeads Prevail\n",
      "                                -\n",
      "                                  Joystiq\n",
      "                                  The Unofficial Apple Weblog (TUAW)\n",
      "                          -\n",
      "                            Autoblog\n",
      "                            -\n",
      "                              Engadget\n",
      "                              TMZ.com\n",
      "                        -\n",
      "                          Matt Cutts: Gadgets, Google, and SEO\n",
      "                          -\n",
      "                            PaulStamatiou.com\n",
      "                            -\n",
      "                              -\n",
      "                                GigaOM\n",
      "                                TechCrunch\n",
      "                              -\n",
      "                                -\n",
      "                                  Techdirt\n",
      "                                  Creating Passionate Users\n",
      "                                -\n",
      "                                  Joho the Blog\n",
      "                                  -\n",
      "                                    -\n",
      "                                      PerezHilton.com\n",
      "                                      Jeremy Zawodny's blog\n",
      "                                    -\n",
      "                                      Joi Ito's Web\n",
      "                                      -\n",
      "                                        ongoing\n",
      "                                        -\n",
      "                                          Joel on Software\n",
      "                                          -\n",
      "                                            -\n",
      "                                              we make money not art\n",
      "                                              -\n",
      "                                                plasticbag.org\n",
      "                                                -\n",
      "                                                  Signal vs. Noise\n",
      "                                                  -\n",
      "                                                    kottke.org\n",
      "                                                    -\n",
      "                                                      Neil Gaiman's Journal\n",
      "                                                      -\n",
      "                                                        -\n",
      "                                                          The Huffington Post | Raw Feed\n",
      "                                                          -\n",
      "                                                            Wonkette\n",
      "                                                            -\n",
      "                                                              Gawker\n",
      "                                                              -\n",
      "                                                                The Superficial - Because You're Ugly\n",
      "                                                                Go Fug Yourself\n",
      "                                                        -\n",
      "                                                          Deadspin\n",
      "                                                          Gothamist\n",
      "                                            -\n",
      "                                              Kotaku\n",
      "                                              Gizmodo\n",
      "                      -\n",
      "                        Shoemoney - Skills to pay the bills\n",
      "                        -\n",
      "                          flagrantdisregard\n",
      "                          -\n",
      "                            WWdN: In Exile\n",
      "                            -\n",
      "                              Derek Powazek\n",
      "                              -\n",
      "                                lifehack.org\n",
      "                                Dave Shea's mezzoblue\n",
      "                -\n",
      "                  Wired News: Top Stories\n",
      "                  -\n",
      "                    Topix.net Weblog\n",
      "                    Bloglines | News\n"
     ]
    }
   ],
   "source": [
    "printclust(clust, labels=blognames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-98,\n",
       " <__main__.bicluster instance at 0x7f44df6b1488>,\n",
       " <__main__.bicluster instance at 0x7f44d83be8c0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust.id, clust.left, clust.right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 对聚类结果进行可视化\n",
    "上面虽然已经打印出了博客聚类的结果，不过还不够直观。书上又重新想画一个树状图来展示聚类结果。这里利用的PIL（Python Image Library）模块，可以尝试用`pip install PIL`进行安装。  \n",
    "我们需要在一个将聚类结果画在一个图上，首先需要获得聚类的总体高度，如果这个聚类没有分支，则高度为1，其他为各分支高度之和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getheight(clust):\n",
    "    if clust.left == None and clust.right == None:\n",
    "        return 1\n",
    "    return getheight(clust.left) + getheight(clust.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getheight(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdepth(clust):\n",
    "    if clust.left == None and clust.right == None:\n",
    "        return 0.0\n",
    "    return max(getdepth(clust.left), getdepth(clust.right)) + clust.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.188162327170954, 0.9960012161819306)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getdepth(clust), clust.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawnode(draw, clust, x, y, scaling, labels):\n",
    "    if clust.id < 0:\n",
    "        h1 = getheight(clust.left) * 20\n",
    "        h2 = getheight(clust.right) * 20\n",
    "        top = y - (h1+h2)/2\n",
    "        bottom = y+(h1+h2)/2\n",
    "        # 线的长度\n",
    "        ll = clust.distance*scaling\n",
    "        # 聚类到其子节点的垂直线\n",
    "        draw.line((x, top + h1/2, x, bottom-h2/2), fill=(255,0,0))\n",
    "        # 连接左侧节点的水平线\n",
    "        draw.line((x, top+h1/2, x+ll, top+h1/2), fill=(255,0,0))\n",
    "        # 连接右侧节点的水平线\n",
    "        draw.line((x, bottom-h2/2, x+ll, bottom-h2/2), fill=(255,0,0))\n",
    "        \n",
    "        # 调用函数绘制左右节点\n",
    "        drawnode(draw, clust.left, x+ll, top+h1/2, scaling, labels)\n",
    "        drawnode(draw, clust.right, x+ll, bottom-h2/2, scaling, labels)\n",
    "    else:\n",
    "        draw.text((x+5, y-7), labels[clust.id], (0,0,0))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawdendrogram(clust, labels, jpeg='clusters.jpg'):\n",
    "    # 这里设置了图片的长宽，depth相当于x轴方向，表示距离的大小\n",
    "    h = getheight(clust) * 20\n",
    "    w = 1200\n",
    "    depth = getdepth(clust)\n",
    "    \n",
    "    scaling = float(w-150)/depth\n",
    "    \n",
    "    img = Image.new('RGB', (w,h), (255,255,255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    draw.line((0, h/2, 10, h/2), fill=(255,0,0))\n",
    "    \n",
    "    drawnode(draw, clust, 10, (h/2), scaling, labels)\n",
    "    img.save(jpeg, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drawdendrogram(clust, blognames, jpeg=\"blogclust.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 对列聚类\n",
    "上面我们对博客进行了聚类，这里我们换一个方向，置换一下数据，对出现的词汇进行聚类，看会有什么结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rotatematrix(data):\n",
    "    return [[data[i][j] for i in range(len(data))] for j in range(len(data[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdata = rotatematrix(data)\n",
    "wordclust = hcluster(rdata)\n",
    "drawdendrogram(wordclust, labels=words, jpeg='wordclust.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 K-means均值聚类  \n",
    "层次聚类的方法有两个缺点：\n",
    "1. 在没有额外投入的情况下，树形图是不会真正将数据分成不同的分组  \n",
    "2. 需要计算两两之间的相似性，计算量比较大    \n",
    "\n",
    "这里K-means聚类就可以避免上面的情况了，在计算前，我们会提前告知需要分成几个小组，然后程序会随机先生成*k*个中心位置，然后将各个数据点分配到最临近的中心点，之后再重新计算中心位置，之后又重新进行新一轮的分配，直到分配结果不会发生变化为止。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def kcluster(rows,distance=pearson,k=4):\n",
    "  # Determine the minimum and maximum values for each point\n",
    "  ranges=[\n",
    "        (min([row[i] for row in rows]),max([row[i] for row in rows])) \n",
    "          for i in range(len(rows[0]))\n",
    "         ]\n",
    "\n",
    "  # Create k randomly placed centroids 生成K个随机数组，这些数据值介于该行最大值和最小值之间\n",
    "  clusters=[\n",
    "        [random.random()*(ranges[i][1]-ranges[i][0])+ranges[i][0] \n",
    "            for i in range(len(rows[0]))] for j in range(k)\n",
    "           ]\n",
    "  \n",
    "  lastmatches=None\n",
    "  # 重复运行100次，直到结果不发生变化为止\n",
    "  for t in range(100):\n",
    "    print 'Iteration %d' % t\n",
    "    bestmatches=[[] for i in range(k)]\n",
    "    \n",
    "    # Find which centroid is the closest for each row\n",
    "    for j in range(len(rows)):\n",
    "      row=rows[j]\n",
    "      bestmatch=0\n",
    "      for i in range(k):\n",
    "        d=distance(clusters[i],row)\n",
    "        if d<distance(clusters[bestmatch],row): \n",
    "          bestmatch=i\n",
    "      bestmatches[bestmatch].append(j)\n",
    "\n",
    "    # If the results are the same as last time, this is complete\n",
    "    if bestmatches==lastmatches: break\n",
    "    lastmatches=bestmatches\n",
    "    \n",
    "    # Move the centroids to the average of their members\n",
    "    for i in range(k):\n",
    "      avgs=[0.0]*len(rows[0])   # 设置都为0.0初始值的平均值变量\n",
    "      if len(bestmatches[i])>0:\n",
    "        for rowid in bestmatches[i]:\n",
    "          for m in range(len(rows[rowid])):\n",
    "            avgs[m]+=rows[rowid][m]\n",
    "        for j in range(len(avgs)):\n",
    "          avgs[j]/=len(bestmatches[i])\n",
    "        clusters[i]=avgs\n",
    "      \n",
    "  return bestmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n"
     ]
    }
   ],
   "source": [
    "kclust = kcluster(data, k =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n"
     ]
    }
   ],
   "source": [
    "kclust1 = kcluster(data, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 4, 9, 10, 2, 5, 2, 11, 7, 5, 2, 0, 6, 11, 5, 2, 3, 4, 4]\n",
      "['Publishing 2.0', 'Mashable!', 'Valleywag', 'Slashdot', 'Topix.net Weblog', 'The Viral Garden', 'Techdirt', 'Creating Passionate Users', \"Seth's Blog\"]\n"
     ]
    }
   ],
   "source": [
    "print [len(x) for x in kclust1]\n",
    "print [blognames[i] for i in kclust1[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果，我们可以看出，对于不同的**K**值都有不同的结果输出。  \n",
    "## 3.5 针对偏好的聚类\n",
    "下面我们开始收集下载用户提交的数据，这些数据包括用户拥有的和希望拥有的东西。下面尝试利用这些信息，把这用户进行分类。    \n",
    "为了从网上获取相关信息，需要利用一个简单的爬虫下载相关的信息。下载的代码使用了`BeautifulSoup`模块来解析HTML格式（书上的模块为旧版的，可以尝试下新版本的BeautifulSoup）。因为**Zebo**可能改版了，直接利用文件**zebo.txt**进行分析。  \n",
    "查看zebo.txt文件可以发现文件中是用0和1表示是否拥有某个物品，这里我们不采用皮尔逊相关性，采用**[Tanimoto系数](https://en.wikipedia.org/wiki/Jaccard_index)**来进行度量。与前面的度量方法不同，Tanimoto更加适合用于[离散型](https://docs.tibco.com/pub/spotfire/6.5.3/doc/html/hc/hc_distance_measures_overview.htm)数值集合的比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanimoto(v1, v2):\n",
    "    c1, c2, shr=0, 0, 0\n",
    "    # 根据共同拥有的东西来判断距离\n",
    "    for i in range(len(v1)):\n",
    "        if v1[i] !=0:\n",
    "            c1 += 1\n",
    "        if v2[i] != 0:\n",
    "            c2 += 1\n",
    "        if v1[i] != 0 and v2[i] !=0:\n",
    "            shr += 1\n",
    "    return 1.0 - float(shr)/(c1+c2-shr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wants, people, data = readfile('zebo.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 500 35\n"
     ]
    }
   ],
   "source": [
    "print len(wants), len(people), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tanimoto_clust = hcluster(data, distance=tanimoto)\n",
    "drawdendrogram(tanimoto_clust, wants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](clusters.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据拥有的物品，按照人群进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdata = rotatematrix(data)\n",
    "rtanimoto_clust = hcluster(rdata, distance=tanimoto)\n",
    "drawdendrogram(rtanimoto_clust, people, jpeg='zebo_people.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](zebo_people.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 以二维形式展现数据\n",
    "这里我们想展示上面计算得到的博客之距离，采用的是**多维缩放**（multidimensional scaling）技术。该技术经常用于多维数据的降维分析，把数据之间的距离或差异性映射到二维平面上点与点之间的距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaledown(data,distance=pearson,rate=0.01):\n",
    "  n=len(data)\n",
    "\n",
    "  # The real distances between every pair of items\n",
    "  realdist=[[distance(data[i],data[j]) for j in range(n)] \n",
    "             for i in range(0,n)]\n",
    "\n",
    "  # Randomly initialize the starting points of the locations in 2D\n",
    "  loc=[[random.random(),random.random()] for i in range(n)]\n",
    "  fakedist=[[0.0 for j in range(n)] for i in range(n)]\n",
    "  \n",
    "  lasterror=None\n",
    "  for m in range(0,1000):\n",
    "    # Find projected distances 寻找投影后的距离\n",
    "    for i in range(n):\n",
    "      for j in range(n):\n",
    "        fakedist[i][j]=sqrt(sum([pow(loc[i][x]-loc[j][x],2) \n",
    "                                 for x in range(len(loc[i]))]))\n",
    "  \n",
    "    # Move points\n",
    "    grad=[[0.0,0.0] for i in range(n)]\n",
    "    \n",
    "    totalerror=0\n",
    "    for k in range(n):\n",
    "      for j in range(n):\n",
    "        if j==k: continue\n",
    "        # The error is percent difference between the distances\n",
    "        errorterm=(fakedist[j][k]-realdist[j][k])/realdist[j][k]\n",
    "        \n",
    "        # Each point needs to be moved away from or towards the other\n",
    "        # point in proportion to how much error it has\n",
    "        grad[k][0]+=((loc[k][0]-loc[j][0])/fakedist[j][k])*errorterm\n",
    "        grad[k][1]+=((loc[k][1]-loc[j][1])/fakedist[j][k])*errorterm\n",
    "\n",
    "        # Keep track of the total error\n",
    "        totalerror+=abs(errorterm)\n",
    "    #print totalerror\n",
    "\n",
    "    # If the answer got worse by moving the points, we are done\n",
    "    if lasterror and lasterror<totalerror: break\n",
    "    lasterror=totalerror\n",
    "    \n",
    "    # Move each of the points by the learning rate times the gradient\n",
    "    for k in range(n):\n",
    "      loc[k][0]-=rate*grad[k][0]\n",
    "      loc[k][1]-=rate*grad[k][1]\n",
    "\n",
    "  return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw2d(data,labels,jpeg='mds2d.jpg'):\n",
    "  img=Image.new('RGB',(2000,2000),(255,255,255))\n",
    "  draw=ImageDraw.Draw(img)\n",
    "  for i in range(len(data)):\n",
    "    x=(data[i][0]+0.5)*1000\n",
    "    y=(data[i][1]+0.5)*1000\n",
    "    #print x,y\n",
    "    draw.ellipse((x-5,y-5,x+5,y+5), fill=(0,0,255)) # 添加小圆点\n",
    "    draw.text((x,y+5),labels[i],(0,0,0))\n",
    "  img.save(jpeg,'JPEG')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "coords = scaledown(data)\n",
    "draw2d(coords, blognames, jpeg='blog2d.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw2d(coords, blognames, jpeg='blog2d1.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
