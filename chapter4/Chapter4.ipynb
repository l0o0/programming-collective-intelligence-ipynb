{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 搜索与排名\n",
    "这一章开始，介绍的内容是全文搜索引擎，可以在大量文档中搜索一系列单词，并根据这些单词的相关程度对结果进行排名。 下面需要完成的任务有：\n",
    "- 搜集文档（爬虫）  \n",
    "- 为文档建立索引  \n",
    "- 结果查询  \n",
    "- 结果查询的优化\n",
    "\n",
    "## 4.1 简单的爬虫程序  \n",
    "爬虫能够方便的帮我们获得一系列的网页文档，它能根据一组待建索引的网页，根据这些网页内部的链接而找到其他页面，对这些页面又采用同样的方法。有点类似蜘蛛网一样，一层一层往外扩散。    \n",
    "这里用到了`urllib2`和`bs4`（BeautifulSoup是旧版）两个库，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3 \n",
    "import urllib2\n",
    "from bs4 import *\n",
    "from urlparse import urljoin\n",
    "\n",
    "\n",
    "ignorewords = set(['the', 'to', 'of', ' and', 'a', 'in', 'is', 'it'])\n",
    "\n",
    "class crawler:\n",
    "    def __init__(self, dbname):\n",
    "        self.con = sqlite3.connect(dbname)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.con.close()\n",
    "    \n",
    "    def dbcommit(self):\n",
    "        self.con.commit()\n",
    "    \n",
    "    def createindextables(self):  # 创建数据库表格和索引\n",
    "        self.con.execute('create table urllist(url)')\n",
    "        self.con.execute('create table wordlist(word)')\n",
    "        self.con.execute('create table wordlocation(urlid,wordid,location)')\n",
    "        self.con.execute('create table link(fromid integer,toid integer)')\n",
    "        self.con.execute('create table linkwords(wordid,linkid)')\n",
    "        self.con.execute('create index wordidx on wordlist(word)')\n",
    "        self.con.execute('create index urlidx on urllist(url)')\n",
    "        self.con.execute('create index wordurlidx on wordlocation(wordid)')\n",
    "        self.con.execute('create index urltoidx on link(toid)')\n",
    "        self.con.execute('create index urlfromidx on link(fromid)')\n",
    "        self.dbcommit()\n",
    "    \n",
    "    def getentryid(self, table, field, value, createnew=True):\n",
    "        cur = self.con.execute(\n",
    "            \"select rowid from %s where %s=%s\" % (table, field, value)\n",
    "            )\n",
    "        res = cur.fetchone()\n",
    "        if res == None:\n",
    "            cur = self.con.execute(\n",
    "                \"insert into %s (%s) value ('%s')\" % (table, field, value)\n",
    "            )\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0]\n",
    "    \n",
    "    def addtoindex(self, url, soup):\n",
    "        if self.isindexed(url):\n",
    "            return\n",
    "        print 'Indexing %s' % url\n",
    "        \n",
    "        # 获取每个单词\n",
    "        text = self.gettextonly(soup)\n",
    "        words = separatewords(text)\n",
    "        \n",
    "        # 得到URL的id\n",
    "        urlid = self.getentryid('urllist', 'url', url)\n",
    "        \n",
    "        # 将单词与url进行关联\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in ignorewords:\n",
    "                continue\n",
    "            wordid = self.getentryid('wordlist', 'word', word)\n",
    "            self.con.execute('insert into wordlocation(urlid, wordid, location) \\\n",
    "                values (%d, %d, %d)' % (urlid, wordid, i))\n",
    "        \n",
    "    def gettextonly(self, soup):\n",
    "        v = soup.string\n",
    "        if v == None:\n",
    "            c = soup.contents\n",
    "            resulttext = ''\n",
    "            for t in c:\n",
    "                subtext = self.gettextonly(t)\n",
    "                resulttext += subtext + '\\n'\n",
    "            return resulttext\n",
    "        else:\n",
    "            return v.strip()\n",
    "    \n",
    "    def separatewords(self, text):   # 将文本划分成单词的程序\n",
    "        splitter = re.compile(r'\\W*')\n",
    "        return [s.lower() for s in splitter.split(text) if s != \"\"]\n",
    "    \n",
    "    def isindexed(self, url):\n",
    "        u = self.con.execute(\n",
    "            \"select rowid from urllist where url='%s'\" % url).fetchone()\n",
    "        if u != None:\n",
    "            v = self.con.execute(\n",
    "                \"select * from wordlocation where urlid=%s\" % u[0]).fetchone()\n",
    "            if v != None:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def addlinkref(self, urlFrom, urlTo, linkText):\n",
    "        pass\n",
    "    \n",
    "    def crawl(self,pages, depth=2):\n",
    "        for i in range(depth):    # 爬取的循环为2，避免过多的深入，造成可能的无限循环\n",
    "            newpages = set()    # 建立一个集合来存储，可以去除重复的url\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c=urllib2.urlopen(page)\n",
    "                except:\n",
    "                    print \"Could not open %s\" % page\n",
    "                    continue \n",
    "                soup = BeautifulSoup(c.read())\n",
    "                self.addtoindex(page, soup)\n",
    "                \n",
    "                links = soup('a')  # html中链接以<a>形式的tag进行标记\n",
    "                for link in links:\n",
    "                    if ('href' in dict(link.attrs)):\n",
    "                        url=urljoin(page, link['href'])\n",
    "                        #print url\n",
    "                        if url.find(\"'\") != -1:   # 当url中出现' 时不需要添加\n",
    "                            continue\n",
    "                        url = url.split('#')[0]  # 去除网页位置信息  \n",
    "                        if url[0:4] == 'http' and not self.isindexed(url):\n",
    "                            newpages.add(url)\n",
    "                        linkText = self.gettextonly(link)\n",
    "                        self.addlinkref(page, url, linkText)\n",
    "            self.dbcommit()\n",
    "        #print \"newpages: \", newpages\n",
    "        pages = newpages\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "disk I/O error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-1718b14f0c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpagelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mu'http://baike.baidu.com/item/%E6%9E%B8%E6%9D%9E'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"searchindex.db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpagelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-720b5509cfa2>\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(self, pages, depth)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddtoindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# html中链接以<a>形式的tag进行标记\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-720b5509cfa2>\u001b[0m in \u001b[0;36maddtoindex\u001b[0;34m(self, url, soup)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maddtoindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misindexed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'Indexing %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-720b5509cfa2>\u001b[0m in \u001b[0;36misindexed\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misindexed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         u = self.con.execute(\n\u001b[0;32m---> 84\u001b[0;31m             \"select rowid from urllist where url='%s'\" % url).fetchone()\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             v = self.con.execute(\n",
      "\u001b[0;31mOperationalError\u001b[0m: disk I/O error"
     ]
    }
   ],
   "source": [
    "pagelist = [u'http://baike.baidu.com/item/%E6%9E%B8%E6%9D%9E']\n",
    "cra = crawler(\"searchindex.db\")\n",
    "cra.crawl(pagelist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 建立索引  \n",
    "上面已经能够由输入的网址而获得一系列相关的网站信息，这些信息保存于`newpages`中。之后需要对获得的网页建立索引，有了索引能够更方便的帮助我们查询包括单词的文档、单词所在位置等信息。对于网页中的内容，需要做这下面几步：\n",
    "- 过滤掉了所有的标点符号  \n",
    "- 忽略所有非文本元素  \n",
    "- 对文本进行分词  \n",
    "\n",
    "索引使用的数据库为SQLite数据库，简单方便，一般python都自带相关的模块（这里使用`sqlite3`与原书不一致）来处理数据库连接。继续对上面的脚本进行修改，添加数据库操作相关的代码。 一共需要添加5个表格，同时各自建立了索引以方便检索：\n",
    "1. link：保存2个链接，指明由哪个链接到哪个链接的关系  \n",
    "2. urllist：保存索引过的url列表  \n",
    "3. wordlocation：保存单词在文档中的位置信息  \n",
    "4. linkwords：表明单词存在于哪个链接中   \n",
    "5. wordlist：保存的是单词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
